{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Neural Network using Iris dataset from scikit-learn\n",
    "## Iris dataset has 50 samples from three species, Iris Setosa, Iris Verginica, and Iris Versicolor\n",
    "## It has four features, length and width of the sepals and petals\n",
    "## Using this feature, will try to classify two species Iris Setosa and Iris Verginica by training a simple neural network. The first hundred samples in the iris data set correspond to Iris setosa, and Iris verginica mapped as 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting first 100 samples for Iris Setosa and Iris Verginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:100, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = iris.target[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    col_max = np.max(data, axis = 0)\n",
    "    col_min = np.min(data, axis = 0)\n",
    "    return np.divide(data - col_min, col_max - col_min)\n",
    "\n",
    "X_norm = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2962963  0.625      0.09756098 0.05882353]\n",
      " [0.22222222 0.41666667 0.09756098 0.05882353]\n",
      " [0.14814815 0.5        0.07317073 0.05882353]\n",
      " [0.11111111 0.45833333 0.12195122 0.05882353]\n",
      " [0.25925926 0.66666667 0.09756098 0.05882353]\n",
      " [0.40740741 0.79166667 0.17073171 0.17647059]\n",
      " [0.11111111 0.58333333 0.09756098 0.11764706]\n",
      " [0.25925926 0.58333333 0.12195122 0.05882353]\n",
      " [0.03703704 0.375      0.09756098 0.05882353]\n",
      " [0.22222222 0.45833333 0.12195122 0.        ]\n",
      " [0.40740741 0.70833333 0.12195122 0.05882353]\n",
      " [0.18518519 0.58333333 0.14634146 0.05882353]\n",
      " [0.18518519 0.41666667 0.09756098 0.        ]\n",
      " [0.         0.41666667 0.02439024 0.        ]\n",
      " [0.55555556 0.83333333 0.04878049 0.05882353]\n",
      " [0.51851852 1.         0.12195122 0.17647059]\n",
      " [0.40740741 0.79166667 0.07317073 0.17647059]\n",
      " [0.2962963  0.625      0.09756098 0.11764706]\n",
      " [0.51851852 0.75       0.17073171 0.11764706]\n",
      " [0.2962963  0.75       0.12195122 0.11764706]\n",
      " [0.40740741 0.58333333 0.17073171 0.05882353]\n",
      " [0.2962963  0.70833333 0.12195122 0.17647059]\n",
      " [0.11111111 0.66666667 0.         0.05882353]\n",
      " [0.2962963  0.54166667 0.17073171 0.23529412]\n",
      " [0.18518519 0.58333333 0.2195122  0.05882353]\n",
      " [0.25925926 0.41666667 0.14634146 0.05882353]\n",
      " [0.25925926 0.58333333 0.14634146 0.17647059]\n",
      " [0.33333333 0.625      0.12195122 0.05882353]\n",
      " [0.33333333 0.58333333 0.09756098 0.05882353]\n",
      " [0.14814815 0.5        0.14634146 0.05882353]\n",
      " [0.18518519 0.45833333 0.14634146 0.05882353]\n",
      " [0.40740741 0.58333333 0.12195122 0.17647059]\n",
      " [0.33333333 0.875      0.12195122 0.        ]\n",
      " [0.44444444 0.91666667 0.09756098 0.05882353]\n",
      " [0.22222222 0.45833333 0.12195122 0.        ]\n",
      " [0.25925926 0.5        0.04878049 0.05882353]\n",
      " [0.44444444 0.625      0.07317073 0.05882353]\n",
      " [0.22222222 0.45833333 0.12195122 0.        ]\n",
      " [0.03703704 0.41666667 0.07317073 0.05882353]\n",
      " [0.2962963  0.58333333 0.12195122 0.05882353]\n",
      " [0.25925926 0.625      0.07317073 0.11764706]\n",
      " [0.07407407 0.125      0.07317073 0.11764706]\n",
      " [0.03703704 0.5        0.07317073 0.05882353]\n",
      " [0.25925926 0.625      0.14634146 0.29411765]\n",
      " [0.2962963  0.75       0.2195122  0.17647059]\n",
      " [0.18518519 0.41666667 0.09756098 0.11764706]\n",
      " [0.2962963  0.75       0.14634146 0.05882353]\n",
      " [0.11111111 0.5        0.09756098 0.05882353]\n",
      " [0.37037037 0.70833333 0.12195122 0.05882353]\n",
      " [0.25925926 0.54166667 0.09756098 0.05882353]\n",
      " [1.         0.5        0.90243902 0.76470588]\n",
      " [0.77777778 0.5        0.85365854 0.82352941]\n",
      " [0.96296296 0.45833333 0.95121951 0.82352941]\n",
      " [0.44444444 0.125      0.73170732 0.70588235]\n",
      " [0.81481481 0.33333333 0.87804878 0.82352941]\n",
      " [0.51851852 0.33333333 0.85365854 0.70588235]\n",
      " [0.74074074 0.54166667 0.90243902 0.88235294]\n",
      " [0.22222222 0.16666667 0.56097561 0.52941176]\n",
      " [0.85185185 0.375      0.87804878 0.70588235]\n",
      " [0.33333333 0.29166667 0.70731707 0.76470588]\n",
      " [0.25925926 0.         0.6097561  0.52941176]\n",
      " [0.59259259 0.41666667 0.7804878  0.82352941]\n",
      " [0.62962963 0.08333333 0.73170732 0.52941176]\n",
      " [0.66666667 0.375      0.90243902 0.76470588]\n",
      " [0.48148148 0.375      0.63414634 0.70588235]\n",
      " [0.88888889 0.45833333 0.82926829 0.76470588]\n",
      " [0.48148148 0.41666667 0.85365854 0.82352941]\n",
      " [0.55555556 0.29166667 0.75609756 0.52941176]\n",
      " [0.7037037  0.08333333 0.85365854 0.82352941]\n",
      " [0.48148148 0.20833333 0.70731707 0.58823529]\n",
      " [0.59259259 0.5        0.92682927 1.        ]\n",
      " [0.66666667 0.33333333 0.73170732 0.70588235]\n",
      " [0.74074074 0.20833333 0.95121951 0.82352941]\n",
      " [0.66666667 0.33333333 0.90243902 0.64705882]\n",
      " [0.77777778 0.375      0.80487805 0.70588235]\n",
      " [0.85185185 0.41666667 0.82926829 0.76470588]\n",
      " [0.92592593 0.33333333 0.92682927 0.76470588]\n",
      " [0.88888889 0.41666667 0.97560976 0.94117647]\n",
      " [0.62962963 0.375      0.85365854 0.82352941]\n",
      " [0.51851852 0.25       0.6097561  0.52941176]\n",
      " [0.44444444 0.16666667 0.68292683 0.58823529]\n",
      " [0.44444444 0.16666667 0.65853659 0.52941176]\n",
      " [0.55555556 0.29166667 0.70731707 0.64705882]\n",
      " [0.62962963 0.29166667 1.         0.88235294]\n",
      " [0.40740741 0.41666667 0.85365854 0.82352941]\n",
      " [0.62962963 0.58333333 0.85365854 0.88235294]\n",
      " [0.88888889 0.45833333 0.90243902 0.82352941]\n",
      " [0.74074074 0.125      0.82926829 0.70588235]\n",
      " [0.48148148 0.41666667 0.75609756 0.70588235]\n",
      " [0.44444444 0.20833333 0.73170732 0.70588235]\n",
      " [0.44444444 0.25       0.82926829 0.64705882]\n",
      " [0.66666667 0.41666667 0.87804878 0.76470588]\n",
      " [0.55555556 0.25       0.73170732 0.64705882]\n",
      " [0.25925926 0.125      0.56097561 0.52941176]\n",
      " [0.48148148 0.29166667 0.7804878  0.70588235]\n",
      " [0.51851852 0.41666667 0.7804878  0.64705882]\n",
      " [0.51851852 0.375      0.7804878  0.70588235]\n",
      " [0.7037037  0.375      0.80487805 0.70588235]\n",
      " [0.2962963  0.20833333 0.48780488 0.58823529]\n",
      " [0.51851852 0.33333333 0.75609756 0.70588235]]\n"
     ]
    }
   ],
   "source": [
    "print(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data in X_norm is of shape (100,4), but for SNN, we need a matrix with(feature, no of samples) format, so we will transpose X_norm to get the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_norm.T\n",
    "Y_data = Y.reshape(1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Weights and bias\n",
    "### initialize the weights and bias to some random values. Since we have four features, the weight vector should be of (4,1) dimension and bias of shape(1,1)\n",
    "### In this case, we will be initializing all these values to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeNetwork(num_features):\n",
    "    W = np.zeros((num_features, 1))\n",
    "    b = 0\n",
    "    parameters = {\"W\": W, \"b\": b}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Activation Function\n",
    "### Since this is binary classification, let's consider a sigmoid function that maps any linear input to values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(X, Y, parameters):\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    Z = np.dot(W.T,X) + b\n",
    "    A = sigmoid(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating the cost function for a given number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(A, Y, num_samples):\n",
    "    return -1/num_samples * np.sum(Y * np.log(A) + (1- Y)*(np.log(1-A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Backpropagation\n",
    "### Using the output A, now we need to find the derivatives of weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backPropagation(X, Y, A, num_samples):\n",
    "    dZ = A - Y\n",
    "    dW = (np.dot(X, dZ.T))/num_samples\n",
    "    db = np.sum(dZ)/num_samples\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subtracting the derivatives from original weights and bias, while doing that, we'll multiply the derivatives with a learning rate to have control over the gradient at each step of iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters(parameters, dW, db, learning_rate):\n",
    "    W = parameters[\"W\"] - (learning_rate * dW)\n",
    "    b = parameters[\"b\"] - (learning_rate * db)\n",
    "    return {\"W\": W, \"b\": b}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, num_iter, learning_rate):\n",
    "    num_features = X.shape[0]\n",
    "    num_samples = float(X.shape[1])\n",
    "    parameters = initializeNetwork(num_features)\n",
    "    for i in range(num_iter):\n",
    "        A = forwardPropagation(X, Y, parameters)\n",
    "        if(i%100 == 0):\n",
    "            print(\"cost after {} iteration\".format(i, cost(A, Y, num_samples)))\n",
    "        dW, db = backPropagation(X, Y, A, num_samples)\n",
    "        parameters = updateParameters(parameters, dW, db, learning_rate)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0 iteration\n",
      "cost after 100 iteration\n",
      "cost after 200 iteration\n",
      "cost after 300 iteration\n",
      "cost after 400 iteration\n",
      "cost after 500 iteration\n",
      "cost after 600 iteration\n",
      "cost after 700 iteration\n",
      "cost after 800 iteration\n",
      "cost after 900 iteration\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_data, Y, 1000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
